# Balancing Privacy and Data Usability: An Overview of Disclosure Avoidance Methods {#discavoid}

```{r, echo=FALSE, results='asis'}
printauthor("discavoid")
```

The purpose of this handbook is to provide guidance on how to enable broader but ethical and legal access to data. Within the "Five Safes" framework [@desai2016], data providers need to create "safe data" that can be provided to trusted "safe users", within "[safe settings](#security)", subject to legal and [contractual safeguards](#dua). Related, but distinct, is the question of how to create "safe outputs" from researchers' findings, before those findings finally make their way into the public through, say, policy briefs or the academic literature. The processes used to create "safe data" and "safe outputs" - manipulations that render data less sensitive and therefore more appropriate for public release - are generally referred to as +statistical_disclosure_limitation| (SDL).^[Other terms sometimes used are "anonymization" or "de-identification," but as this chapter will show, de-identification is a particular method of SDL, and anonymization is a goal, never fully achieved, rather than a method.] In this chapter, we will describe methods traditionally used within the field of SDL, pointing at methods as well as metrics to assess the resultant statistical quality and sensitivity of the data. Newer methods, generally referred to as "formal privacy methods," are described in a separate [chapter](#diffpriv). 

At their core, SDL methods prevent outsiders from learning "too much" about any one record in the data [@dalenius_towards_1977] by deliberately, and judiciously, adding distortions. Ideally, these distortions maintain the validity of the data for statistical analysis, but strongly reduce the ability to isolate records and infer precise information about individual people, firms, or cases. In general, it is necessary to sacrifice validity in order to prevent disclosure [@goroff_balancing_2015; @abowd_economic_2015]. It is therefore important for data custodians to bear this tradeoff in mind when deciding whether and how to use SDL. 

One key challenge for implementing privacy systems lies in choosing the amount, or type, of privacy to provide. Answering this question requires some way to understand the individual and social value of privacy. @abowd_economic_2019 discuss the question of optimal privacy protection (see also @hsu_differential_2014 in the specific context of differential privacy). For an illustration, see @spencer_effects_2015, who use a calibration exercise to study the costs, measured in misallocated congressional seats, of reduced accuracy in population census data. 

Part of the social value of privacy arises from its relationship to scientific integrity. While the law of information recovery suggests that improved privacy must come at the cost of increased error in published statistics, these effects might be mitigated through two distinct channels. 
First, people may be more truthful in surveys if they believe their data is not at risk, as @couper_risk_2008 illustrate. Second, work in computer science and statistics [@dwork_generalization_2015; @dwork_fienberg_2018; @cummings_adaptive_2016] suggests a somewhat surprising benefit of differential privacy: protection against overfitting. 

There are three factors that a data custodian should bear in mind when deciding whether and how to implement an SDL system in support of making data accessible. First, it is necessary to clarify the specific privacy requirements based on the nature of the underlying data, institutional and policy criteria, and ethical considerations. In addition, the custodian, perhaps in consultation with users, should clarify what sorts of analyses the data will support. Finally, SDL is often part of a broader system to protect sensitive data that can also involve access restrictions and other technical barriers. The broader system may allow for less stringent SDL techniques when providing data to researchers in secure environments than would be possible if data were to be released as unrestricted public use data.^[The [chapter on the RDC-IAB](#iab) provides a good illustration of how various SDL methods are combined with different access methods to provide multiple combinations of analytic validity and risk of disclosure.] This implies in particular that we will not provide a recommendation for a "best" method, since no such globally optimal method exists in isolation.

Rather, this chapter provides an overview of the concepts and more widely-used methods of SDL. Relative to other primers that cover similar material, we focus more closely on the advantages and disadvantages of various methods from the perspective of data users. This chapter can serve as a reference that data providers and data users can use to discuss which forms of SDL are appropriate and will satisfy the needs of both parties. In particular, we focus on how common SDL tools affect different types of statistical analysis as well as the kind of confidentiality protections they support, drawing heavily on @abowd_economic_2015. SDL is a broad topic with a vast literature, starting with @fellegi_question_1972. Naturally, this brief summary is not a replacement for the textbook treatment of SDL in @duncan_statistical_2011. Finally, SDL methods must be implemented and deployed, and we provide pointers to existing "off-the-rack" tools in a variety of platforms (Stata, R, and Python). Readers might also consult other summaries and guides, such as @dupriez_dissemination_2010, @world_bank_dime_nodate, @kopper_j-pal_2020, and @liu_statistical_2020.

## Purpose of Statistical Disclosure Limitation Methods: Definitions and Context

<!-- (1-1.5 pages) -->

A clear and precise sense of what constitutes an unauthorized disclosure is a prerequisite to implementing SDL. Are all data items equally sensitive? How much more should one be able to learn about certain classes of people, firms, villages, etc.? Note that even when "trusted researchers" ("safe people") can be sworn to secrecy, the ultimate goal is to publish using information gleaned from the data, and the final audience can never be considered trusted.^[In the United States, 62% of individuals are aware (and possibly resigned) that government and private companies collect data on them, and seem to believe that there is little benefit to them of such collection: 81% think so when companies do the data collection, and 66% when the government does so [@auxier_americans_2019].]

The key concepts are privacy and confidentiality. +Privacy| can be viewed, in this context, as the right to restrict others' access to personal information, whether through query or through observation [@hirshleifer_privacy_1980]. +Confidentiality| pertains to data that have already been collected, and describes the principle that the data should not be used in ways that could harm the persons that provided it. 

> For example, Ann, who is asked to participate in a study about health behaviors, has a *privacy* right to refuse to answer a question about smoking. If she does answer the question, it would breach *confidentiality* if her response was then used by an insurance company to adjust her premiums [@duncan_private_1993].

@harris-kojetin_statistical_2005 define "disclosure" as the "inappropriate attribution of information to a data subject, whether an individual or an organization" [@harris-kojetin_statistical_2005, p. 4], and describe three different types of disclosure. An *identity disclosure* is one where it is possible to learn that a particular record or data item belongs to a particular participant (individual or organization). An *attribute disclosure* happens if publication of the data reveals an attribute of a participant. Note that an *identity disclosure* necessarily entails attribute disclosure, but the reverse is not the case. 

> In our hypothetical health study, if Ann responds that she is a smoker, an identity disclosure would mean someone can determine which record is hers, and therefore can also learn that she is a smoker -- an attribute disclosure. However, an attribute disclosure could also occur if someone knows that Ann was in the study, they know that Ann lives in a particular zip code, and the data reveal that all participants from that zip code are also smokers. Her full record is not revealed, but confidentiality was breached all the same.

With these concepts in mind, it is necessary to ask whether it is sufficient to prevent blatant "all-or-nothing" identity or attribute disclosures. Usually not, as it may be possible to learn a sensitive attribute with high, but not total, certainty. This is called an *inferential disclosure* [@dalenius_towards_1977; @duncan_disclosure-limited_1986]. 

> Suppose Ann's health insurer knows that Ann is in the data, and that she lives in a particular zip code. If the data have 100 records from that zip code and 99 are smokers, then the insurer has learned Ann's smoking status with imperfect, but high precision.

In addition to deciding what kinds of disclosure can be tolerated and to what extent, in many cases it may also be meaningful to decide which characteristics are and are not sensitive. Smoking behavior may nowadays be regarded as sensitive, but depending on the context, gender might not be. Or, in the case of business data, total sales volume or total payroll are highly sensitive trade secrets. Generally, the county in which the business is located, or the industry in which it operates might not be, but consider a survey of self-employed business people - the location of the business might be the home address, which might be considered highly sensitive. These decisions on what is sensitive affect the implementation of a privacy protection system.^[There is a large and robust literature in economics on the value of privacy. For an overview of ideas in this literature, we recommend @varian_economic_2002 and @acquisti_economics_2016.]

However, additional care must be taken because variables that are not inherently sensitive can still be used to isolate and identify records. Such variables are sometimes referred to as *+quasi-identifiers|* and they can be exploited for *+reidentification|* attacks. In business data, if the data show that there is only one firm operating in a particular county and sector, then their presence inherently leads to identity disclosure. Many of the traditional approaches to SDL operate in large part by preventing re-identification.^[Thus the occasional reference to methods as *deidentification* or *anonymization*, though these terms can sometimes be misleading in regard to what they can actually achieve.] @garfinkel_-identification_2015 discusses techniques for de-identifying data and the many ways in which modern computing tools and a data-rich environment may render effective de-identification impossible, reinforcing the growing need for formal privacy models like differential privacy.

<!-- > Confidential should mean that the dissemination of data in a manner that would allow public identification of the respondent or would in any way be harmful to him is prohibited and that the data are immune from legal process. (SOURCE?) -->

SDL methods may be required for legal and ethical reasons. +Institutional_Review_Boards| (IRBs) require that individual's well-being be protected (see the [chapter on IRBs](#irb)). Legal mandates may intersect with ethical concerns, or prescribe certain (minimal) criteria. Thus, the U.S. +Health_Insurance_Portability_and_Accountability_Act| of 1996 (HIPAA) [@us_department_of_health__human_services_health_nodate] has precise definitions of variables that need to be removed in order to comply with the law's mandate of deidentification [@department_of_health_and_human_services_methods_2012]. The European +General_Data_Protection_Regulation| (GDPR) came into effect in 2018, and has defined both broadly the way researchers can access data, and more narrowly the requirements for disclosure limitation [@cohen_towards_2020; @greene_adjusting_2019; @molnar-gabor_germany_2018]. Similar laws are emerging around the world, and will define both minimal requirements and limits of SDL and other access controls. The +California_Consumer_Privacy_Act| (CCPA) [@marini_comparing_2018] and the Brazilian +Lei_Geral_de_Proteção_de_Dados| (LGDP) [@black_6_2020] came into effect in 2020, and India is currently considering such a law [@panakal_indias_2019].

We note, finally, that there is a parallel concept of non-statistical disclosure limitation that is a complementary part of secure data dissemination. This applies to the metadata---like codebooks, data descriptions, and other summary information---that can leak potentially sensitive information. For example, data documentation might reveal that only certain geographic areas were included in a particular collection, information that could be used as an element in a re-identification attack. While typically not considered quantitative disclosure avoidance, some of the same concepts we will describe here can apply to such metadata as well. For instance, removing mention of the collection area from the documentation is akin to [suppression][suppression], while only revealing broad regions of data collection is akin to [coarsening][coarsening].

## Methods

There are many different SDL methods, and the decision of which to use depends on what needs to be protected, how their use will affect approved analyses, and their technical properties. At a very high level, we can think of an SDL system as a mechanism that takes the raw confidential data, $D$, as an input and produces a modified dataset, $\tilde{D}$. The researcher then conducts their analysis with the modified $\tilde{D}$. Ideally, the researcher can do their analysis as planned, but the risk of disclosure in $\tilde{D}$ is reduced.

Researchers generally need to consider all of the design features that went into producing the data used for an analysis. Most already do so in the context of surveys, where design measures are incorporated into the analysis, often directly in software packages. Some of these adjustments may already take into account various SDL techniques. Traditional survey design adjustments can take into account [sampling][sampling]. Some forms of [coarsening][coarsening] may already be amenable to adjustment using various clustering techniques, such as @moulton_random_1986 (see @cameron_practitioners_2015).

More generally, the inclusion of edits to the data done in service of disclosure limitation is less well supported, and less well integrated into standard research methods. @abowd_economic_2015 argue that the analysis of SDL-laden data is inherently compromised because the details of the SDL protections cannot be disclosed. If they cannot be disclosed, their consequences for inference are unknowable, and, as they show, may be substantial. Regression models, regression discontinuity designs, and instrumental variables models are, generally speaking, affected when SDL is present. The exact nature of any bias or inconsistency will depend on whether SDL was applied to explanatory variables, dependent variables, instruments, or all of the above. Furthermore, it is not always the case that SDL induces an attenuating bias.

With these goals in mind, following @abowd_economic_2015 we distinguish between *ignorable* and *non-ignorable* SDL systems. 
Briefly, SDL is *ignorable* for a particular analysis if the analysis can be performed on the modified data, $\tilde{D}$ as though it were the true data. In a non-ignorable analysis, the result differs in some material way when $\tilde{D}$ is substituted for $D$. When the SDL method is *known*, then it may be possible for the researcher to perform an *SDL-aware* analysis that corrects for non-ignorability. However, SDL methods are generally not ignorable except in certain specific applications.

We briefly outline several of the methods most commonly used within national statistical offices. For interested readers, @harris-kojetin_statistical_2005 describe how SDL systems are implemented in the U.S. statistical system,^[As of the writing of this chapter in August 2020, WP22 is being revised and updated, but has not yet been published.] while @dupriez_dissemination_2010 offers a more multinational perspective.

### De-Identification

In general, it is good practice to remove any variables from the data that are not needed for data processing or analysis, and that could be considered direct identifiers. What constitutes "direct identifiers" may differ on the context, but generally comprises any variable that might directly link to confidential information: names, account or identifier numbers, and sometimes exact birth dates or exact geo-identifiers.^[See guidance in @world_bank_dime_nodate and @kopper_j-pal_2020.] HIPAA defines sixteen identifiers that must be removed in order to comply with the law. It may be necessary to preserve identifiers through parts of the data processing or analysis if they are key variables needed for record linking. In field experiments, the identities of treatment and control units may need to be merged with an administrative dataset. It is also sometimes necessary to use direct identifiers to link records between surveys and administrative data, or precise geographic coordinates may be needed to compute distances as part of the analysis. If possible, the data provider should facilitate record linking while the data are secure, before they are shared with the research team.

### Suppression

<!-- Also mention rules that point to "suppress regression coefficients when derived from less than x% of sample, or the p-percent rule" as they are applied to "safe output" rules in RDC and MOU scenarios. -->

Suppression is perhaps the most common form of SDL, and one of the oldest [@fellegi_question_1972]. In their most basic form, suppression rules work as follows:

1. Model the sensitivity of a particular data item, table cell, or observation ("disclosure risk")
2. Do not allow the release of data items that have excessive disclosure risk (primary suppression)
3. Do not allow the release of other data from which the sensitive item can be calculated (complementary suppression)

Suppression rules can be applied to microdata, in which case the sensitive observations are removed from the microdata, or to tabular data, where the relevant cells are suppressed. 

In the case of business microdata, a firm that is unique in its county and industry might be flagged as having high disclosure risk and eliminated from the data. Another less damaging possibility is that just its sensitive attributes are suppressed, so a researcher would still know that there was a firm operating in that industry and location; just not what its other attributes were. For tabular data, the principle is the same. Continuing with the business application, suppose there is one large firm and several smaller competitors in a given industry and location. If the cell is published, it might be possible for its local competitors to learn the receipts of the dominant firm to a high degree of precision. 

Cell suppression rules based on this sort of reasoning are called *p*-percent rules, where *p* describes the precision with which the largest firm's information can be learned. A conservative estimate of this occurs when the largest firm's value is *(1-p)%* of the cell's value.

A variant of this rule takes into account prior precision *q* (the "pq percent rule"). Another rule is known as the *n,k* rule: a cell is suppressed if *n* or fewer entities contribute *k* percent or more of the cell's value. These rules are frequently applied to statistics produced by national statistical agencies [@harris-kojetin_statistical_2005]. Simpler rules based entirely on cell counts are also encountered, for instance in the Health and Retirement Study [@health_and_retirement_study_disclosure_nodate]. Tables produced using HRS confidential geo-coded data are only allowed to display values when the cell contains three or more records (five for marginal cells). 

If a cell in a contingency table is suppressed based on any one of these rules, it could be backed out by using the information in the table margins and the fact that table cells need to sum up to their margins. Some data providers therefore require that additional cells are suppressed to ensure this sort of reverse engineering is not possible. Figuring out how to choose these *complementary suppressions* in an efficient manner is a non-trivial challenge.

In general, cell suppression is not an ignorable form of SDL. It remains popular because it is easy to explain and does not affect the un-suppressed cells. 

Data suppression is clearly non-ignorable, and is quite difficult to correct for suppression in an SDL-aware analysis.^[One approach is to replace suppressed cells with imputed values, and then treat the data as multiply-imputed.] The features of the data that lead to suppression are often related to the underlying phenomenon of interest. @chetty_practical_2019 provide a clear illustration. They publish neighborhood-level summaries of intergenerational mobility based on tax records linked to Census data. The underlying microdata are highly sensitive, and to protect privacy they used a variant of a differentially privacy model. They show that if they had instead used a cell suppression rule, the published data would be misleading with respect to the relationship between neighborhood poverty and teen pregnancy because both variables are associated with neighborhood population. Hence, the missingness induced by cell suppression is not ignorable.

Suppression can also be applied to model-based statistics. For instance, after having run a regression, coefficients that correspond to cells with fewer than *n* cases may be suppressed. This most often occurs when using dichotomous variables (dummy variables), which represent conditional means for particular subgroups. 

> In a regression, a researcher includes a set of dummies for interacting occupation and location. When cross-tabulating occupation and location, many cells have less than 5 observations contributing to the coefficient. The data provider requires that these be suppressed.

### Coarsening

Coarsening takes detailed attributes that can serve as quasi-identifiers and collapses them into a smaller number of categories. Computer scientists call this "generalizing", and it is also sometimes referred to as "masking". Coarsening can be applied to quasi-identifiers, to prevent re-identification, or to attributes, to prevent accurate attribute inference. When applied to quasi-identifiers, the concern is that an outsider could use detailed quasi-identifiers to single-out a particular record and learn who it belonged to. By coarsening quasi-identifiers, the set of matching records is increased, raising uncertainty about any re-identified individual's true identity. In principle, all variables can serve as quasi-identifiers, and the concept of *k-anonymity* introduced by @sweeney_achieving_2002 is a useful framework for thinking about how to implement coarsening and other microdata SDL. We discuss k-anonymity in the section on [disclosure risk][disclosure risk].

Coarsening is common in microdata releases. As a general rule of thumb, it may make sense to consider coarsening variables with heavy tails (earnings, payroll), residuals (truncate range, suppress labels of range). 
In public-use microdata from the American Community Survey, geographic areas are coarsened until all such areas represent at least 100,000 individuals [@us_census_bureau_finalpublic_2011]. In many data sources, characteristics like age and income, are reported in bins even when the raw data are more detailed. Topcoding is a common type of coarsening, in which variables, e.g. incomes, above a certain threshold are replaced with some topcoded value (e.g., \$200,000 in the Current Population Survey). When releasing model-based estimates, rounding, another form of coarsening, can satisfy statistical best practice -- not releasing numbers beyond their statistical precision -- as well as disclosure avoidance principles -- by preventing inferences that could be too precise about specific records in the data.

Whether coarsening is ignorable or not depends on the analysis to be performed. Consider the case in which incomes are topcoded above the 95th percentile. This form of SDL is ignorable with respect to estimating the 90th percentile of the income distribution (and all other quantiles below the 95th). However, coarsening age is not ignorable if the goal is to conduct an analysis of behavior of individuals right around some age or date-of-birth cutoff. Coarsening rules should therefore bear in mind the intended analysis for the data and may be usefully paired with restricted-access protocols that allow trusted researchers access to the more detailed data. See @burkhauser_estimating_2011 for an example of the impact of top-coding on estimates of earnings inequality.

### Swapping

The premise behind the technique of *swapping* is similar to suppression. Again, each record is assigned a level of disclosure risk. Then, any high-risk record is matched to a less risky record on a set of key variables, and all of the other non-key attributes are swapped. The result is a dataset that preserves the distribution among all the key variables used for matching.
If the original purpose of the data was to publish cross-tabulations of the matching variables, swapping can produce microdata that are consistent with those tabulations.
This approach is more commonly used in censuses and surveys of people or households, and rarely used with establishment data.

> For example, consider our hypothetical health study again, and now suppose we know Ann's zip code, gender, race, ethnicity, age, smoking behavior, and the size of her household. Ann's record might be classified as high risk if, say, she has a very large household relative to the rest of the other respondents who are also from her zip code. If the data are used to publish, say, summaries of smoking behavior by age, race, and gender, then Ann's record would be matched to another record with the same age, race, gender and smoking behavior, and the values of the household size and zip code attributes would be swapped.

Swapping is ignorable for analyses that only depend on the matching variables, since the relationships among them will be preserved. However, swapping distorts relationships among the other variables, and between the matching variables and the other variables. In the example above, the swapping would be non-ignorable in the context of a study of how smoking behavior various across zip codes. In general, statistical agencies are not willing to publish detailed information about how swapping is implemented since that information could be used to reverse-engineer some of the swaps, undoing the protection. Hence, SDL-aware analysis may not be possible, and inference validity negatively affected.

### Sampling

Sampling is the original SDL technique. Rather than the full confidential microdata, publishing a sample inherently limits the certainty with which an attackers can re-identify records.
While sampling can provide a formal privacy guarantee, in modern, detailed surveys, sampling will not in general prevent re-identification. In combination with other tools, like coarsening, sampling may be particularly appealing because, while it is non-ignorable, researchers can adjust their analysis for the sampling using familiar methods. Sampling is often used in conjunction with other methods, including with formally private methods, to amplify the protection provided. 

### Noise Infusion

Noise infusion can refer to an array of related methods, all of which involve distorting data with randomly distributed noise. There is a key distinction between methods where the microdata are infused with noise (+input_noise_infusion|), versus methods where noise is added to functions or aggregates of the data before publication (+output_noise_infusion|).

Noise infusion was developed as a substitute for cell suppression as an approach to protecting tabular summaries of business data. Originally proposed by @evans_using_1998, the basic approach assigns each microdata unit (a business establishment) a multiplicative noise factor drawn from a symmetric distribution (e.g., centered on one), and multiplies sensitive (or all) characteristics by that factor. Tabular summaries can then be made from the distorted characteristics. As cell sizes increase, the distortions applied to each unit average out. Thus, while small cells may be quite distorted and thus protected, large cells usually have little distortion. Most cells no longer need to be suppressed. These approaches are used in the U.S. Census Bureau's Quarterly Workforce Indicators [@abowd_lehd_2009 ; @abowd_dynamically_2012] and County Business Patterns, with a truncated distribution. When the noise distribution is unbounded, for instance Gaussian, noise infusion may be differentially private, see the [chapter on differential privacy](#diffpriv).

Noise infusion has the advantage that it mostly eliminates the need to suppress sensitive records or cells, allowing more information to be revealed from the confidential data while maintaining certain confidentiality protections. Noise infusion also generally preserves the means and covariances among variables. However, it will always inflate estimated variances and can lead to bias in estimates of statistical models, and in particular regression coefficients. Hence, noise infusion is, in general, not ignorable. If the details of the noise distribution can be made available to researchers, then it is possible to correct analysis for noise infusion. However, information about the noise distribution can also help an attacker reverse engineer the protections.

### Synthetic Data and Multiple Imputation

Synthetic data generation and multiple imputation are closely related. In fact, one particular variant of synthetic data as SDL -- partially synthetic data -- is also known as "suppress and impute" [@little_statistical_1993]. Sensitive values for some or all records are replaced by (multiple) imputations. More generally, fully synthetic data [@rubin_discussion_1993] replaces all values with draws from a posterior predictive distribution, estimated given the confidential data. For an overview, see @raghunathan_multiple_2003, @little_statistical_2004, and @drechsler_synthetic_2011.

Synthetic data have been used in the Federal Reserve Board's Survey of Consumer Finances to protect sensitive income values [@kennickell_multiple_1998], and in the American Community Survey microdata to protect data from group quarters [such as prisons and university residences; see @hawala_disclosure_2009]. The U.S. Census Bureau's LODES data, included in the [OnTheMap](https://onthemap.ces.census.gov/) application, uses synthetic household data [@machanavajjhala_privacy_2008]. Synthetic data can be used in conjunction with validation servers: researchers use the synthetic data to create complex model-based estimation, then submit their analysis to a remote server with access to the confidential data for validation of the results. Such a mechanism has been used by the U.S. Census Bureau in collaboration with Cornell University for confidential business microdata [@kinney_towards_2011] and for survey data combined with administrative data [@abowd_final_2006]. The term is sometimes used as well for "test" data for remote submission systems, which typically makes no claims as to the validity - it is simply constructed to replicate the data schema of the confidential data, to test statistical code.

### Some Examples

Table \@ref(tab:overviewtable) shows how the various methods can be combined, drawing on examples both from this Handbook as well as from other frequently used data sources. 

```{r overviewtable, echo=FALSE}
#library(readODS)
overview <- head(sapply(read_ods("./assets/discavoid/summary_methods.ods"),as.character),12)
overview[is.na(overview)] <- " "
kable(overview, caption = "Summary of SDL Methods", format = "html") %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```
## Metrics

<!-- How do you measure risk, and the reduction in risk achieved by applying above methods? Mention uniqueness criteria, k-anonymity, l-diversity, matching metrics, etc. -->

The design of an SDL system depends on determinations about what constitutes an acceptable level of disclosure risk, balanced with the proposed uses of the data. There are many different ways to describe and measure disclosure risk. These share in common a sense of how unique a record or combination of attributes is in the data, which corresponds intuitively to a sense of how easy it would be to single-out that record and re-identify the respondent, perhaps aided by a linked dataset. Likewise, there are many different ways to assess whether the released data are suitable, or fit, for their intended use. These quality measures are often based on how closely the released data match the true data on certain statistical summaries, and it will be important for researchers and data custodians to agree on what are the most relevant summaries.

### Disclosure Risk

Early definitions of disclosure risk were based on rules and guidelines derived from institutional knowledge, assessment of summary measures, and reidentification experiments (@harris-kojetin_statistical_2005). Statisticians have subsequently developed more formal models to measure risk of re-identification for specific types of publication and with particular threat models. For instance, @shlomo_assessing_2010 model reidentification risk in survey microdata when an attacker is matching on certain categorical variables.

Recently, computer scientists and statisticians have introduced more general concepts of disclosure risk and data privacy. Latanya Sweeny proposed the concept of $k$-anonymity [@sweeney_achieving_2002] which defines disclosure risk in terms of the number of records that share the same combination of attributes. If a single record is uniquely identified by some combination of attributes, disclosure risk is high. Sweeny says that a dataset can be called $k$-anonymous if for all feasible combinations of attributes, at least $k$ records have that combination. Intuitively, increases in $k$ reduce the risk that observations can be singled out by linking other datasets that contain the same attributes.
The concept of $k$-anonymity can provide some guidance when thinking about how to implement the SDL systems described above. For example, if records are uniquely identified by age, race, and gender, then one might collapse age into brackets until there are at least $k>1$ records for each such combination.

However, $k$-anonymity does not protect against attribute disclosure. If all $k$ observations with the same combination of attributes also share the same sensitive attribute, say smoking behavior, then the published data do not fully prevent disclosure of smoking behavior. Recognizing this, @machanavajjhala_l-diversity_2007 introduce the concept of $\ell$-diversity. The idea is that whenever a group of records are identical on some set of variables, there must be a certain amount of heterogeneity in important sensitive traits. If a certain group of records match on a set of quasi-identifiers and also all share the same smoking status, then to achieve $\ell$-diversity, we might alter the reported smoking behavior of some fraction ($\ell$) of the records - a form of noise infusion.

### Data Quality

When the released data or output is tabular (histograms, cross-tabulations) or is a limited set of population or model parameters (means, coefficients), a set of distance-based metrics (so-called "$\ell_p$ distance" metrics) can be used to compare the quality of the perturbed data. Note that this is a specific metric, as it is limited to those statistics taken into account - the data quality may be very poor in non-measured attributes! For $p=1$, the $\ell_1$ distance is the sum of absolute differences between the confidential and perturbed data. For $p = 2$, the $\ell_2$ distance is the sum of squared differences between the two datasets (normalized by $n$ the number of observations, it is the Mean Squared Error, MSE).

In settings where it is important to measure data quality over an entire distribution, the Kullbach-Leibler (KL) divergence measure can also be used. The KL-divergence is related to the concept of entropy from information theory and, loosely, measures the amount of surprise associated with seeing an observation drawn from one distribution when you expected them to come from another distribution. Other metrics are based on propensity scores [@woo_global_2009; @snoke_general_2018]. More specific measures will often compare specific analysis output, a task that is quite difficult to conduct in general. @reiter_verification_2009 propose to summarize the difference between regression coefficients when analyses can be run on both confidential and protected data,in the context of verification servers.

## Tools

For data providers faced with the need to start providing safe data for use by external researchers, a growing number of software packages are available that implement the methods described in this chapter. The ICPSR has a checklist that may be of use in early development of an SDL system [@icpsr_disclosure_2020]. The listing of tools below is incomplete, but will provide practitioners with a place to start. A fully-developed SDL system will have unique requirements and may require custom programming. Nevertheless, many tools are useful across a wide range of applications.

Statistics Netherlands maintains the ARGUS software for SDL [@hundepool_argus_1998], including τ-ARGUS to protect tabular data [@de_wolf_-argus_2018], and μ-ARGUS for protecting microdata [@hundepool_-argus_2018]. The software appears to be widely used in statistical agencies in Europe. An open-source R package, `sdcMicro` implements a full suite of tools needed to apply SDL, from computation of risk measures, including $k$-anonymity and $l$-diversity, to implementation of SDL methods and the computation of data quality measures [@templ_statistical_2015 ; @templ_sdcmicro_2020]. 

Simpler tools, focusing on removing direct identifiers, can be found at J-PAL for Stata ([stata_PII_scan](https://github.com/J-PAL/stata_PII_scan)) and R ([PII-scan](https://github.com/J-PAL/PII-Scan)), and at Innovations for Poverty Action for Python or Windows ([PII_detection](https://github.com/PovertyAction/PII_detection)) [@j-pal_j-palstata_pii_scan_2020; @j-pal_j-palpii-scan_2020; @innovations_for_poverty_action_povertyactionpii_detection_2020]. 

A number of R packages facilitate generation of synthetic data. @raab_practical_2016 and @nowok_synthpop_2016 provide `synthpop`, a flexible and up-to-date package with methods for generating synthetic microdata. The R package `simPop` @templ_simpop_2019 can also generate synthetic populations from aggregate data, which can be useful for testing SDL systems on non-sensitive data. In some cases, one might also consider using general-purpose software for multiple imputation for data synthesis.^[See "[Multiple imputation in Stata](https://stats.idre.ucla.edu/stata/seminars/mi_in_stata_pt1_new/)" or the `mice` package in R [@buuren_mice_2011].]

Many of the methods described in this chapter are technical and require statistical and programming expertise. If that expertise is not already available among staff, some institutions provide guidance to researchers who wish to apply SDL techniques.

## Conclusion

There is a greater demand than ever for all kinds of data. More than ever before, scholars and analysts have the tools to use data to better understand the economy and society, and to inform policy. Alongside these advances, data custodians find themselves under pressure to make databases available to outsiders. However, the pressure to make data available is not always accompanied by the resources, tools, or expertise needed to do so safely.

The same advances driving these new demands have a darker side. Computing power together with the availability of detailed outside data make it easier than ever for attackers to exploit improperly protected data. Therefore, when making data available for research, agency stewards must take great care to also protect the subjects in the data. This chapter provides an overview of techniques traditionally used to modify the data to achieve that goal. There is a legitimate concern that some of the methods discussed here cannot protect against all possible attacks made possible with modern computing power. Those concerns animate the discussion of formal methods that yield provable privacy guarantees elsewhere in this Handbook. 

## About the Authors {-}

[Ian M. Schmutte](http://ianschmutte.org/) is Associate Professor in the Department of Economics at the University of Georgia. Schmutte is currently working with the U.S. Census Bureau on new methods for protecting confidential data. His research has appeared in the American Economic Review, Journal of Labor Economics, Journal of Human Resources, Journal of Business and Economic Statistics, and the Brookings Papers on Economic Activity.

[Lars Vilhuber](https://www.vilhuber.com/lars/) is the Executive Director of the [Labor Dynamics Institute](http://www.ilr.cornell.edu/ldi "LDI") at [Cornell University](http://www.cornell.edu "Cornell University"). He has worked for many years with the [Research and Methodology Directorate](https://www.census.gov/research/) at the U.S. Census Bureau on a variety of projects, including implementing disclosure avoidance techniques. He is a member of governing or scientific committees of secure data access center in Canada ([CRDCN](https://crdcn.org/) and France ([CASD](http://casd.eu), and a member of the [American Statistical Association](https://www.amstat.org/)‘s [Committee on Privacy and Confidentiality](https://community.amstat.org/cpc/home). He is the inaugural Data Editor for the [American Economic Association](https://www.aeaweb.org/), and the Managing (Executive) Editor of the [Journal of Privacy and Confidentiality](https://journalprivacyconfidentiality.org/). He is the [Co-Chair, Innovations in Data and Experiments for Action Initiative (IDEA)](https://www.povertyactionlab.org/initiative/innovations-data-experiments-action) at [J-PAL](https://www.povertyactionlab.org/).

## Acknowledgements {-}

This chapter draws on @abowd_introductory_2019 and the INFO7470 class at Cornell University [@abowd_session_2016]. We gratefully acknowledge the support of Alfred P.\ [Sloan Foundation Grant G-2015-13903](https://sloan.org/grant-detail/6845) and [NSF Grant SES-1131848](http://www.nsf.gov/awardsearch/showAward.do?AwardNumber=1131848) for the earlier work. 

## Disclaimer {-}

The views expressed in this paper are those of the authors and not those of the U.S. Census Bureau or other sponsors.



\putbib


